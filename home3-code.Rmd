---
title: "home3-code"
author: "Ting-Yen Tsai"
date: "5/8/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r prep}
source("D:\\Users\\user\\Desktop\\test-data.R")
```

## Problem 1 (a)

```{r 1a}
# helper function
truncated <- function (a, b){
  return (max(a - b, 0));
}
# main function
truncated.power.design.matrix <- function(x) {
  n <- length(x)
  D <- matrix(, nrow = n, ncol = n-1)
  for (i in 1:n){
    for (j in 1:(n-1)){
      D[i, j] <- truncated(x[i], x[j])
    }
  }
  intercept <- rep(1, n)
  cbind(D, intercept)
}
X <- truncated.power.design.matrix(x)

```


## Problem 1(b)
```{r 1b}
library(leaps)
regsubsets.fitted.values <- function(X, regsubsets.out, nterm){
  coefs <- coef(regsubsets.out, id = nterm)
  dim(coefs) <- c(1, nterm)
  reg_col <- as.numeric(summary(regsubsets.out)$which[nterm,])
  selected_col <- which(reg_col %in% 1)
  X_model <- X[,selected_col]
  return (coefs %*% t(X_model))
}
```

## Problem 1 (c)
```{r 1c}
library(ggplot2)
# function for calculating residual sum of squares
rss <- function(y, yhat){
  answer <- 0
  for (i in 1:length(y)){
    answer <- answer + (y[i] - yhat[i])^2
  }
  return(answer)
}
residual_sum_of_squares <- rep(0, 100)
for (k in 1:100){
  reg <- regsubsets(X, y, method = "forward", nvmax = k, intercept = F)
  yhat <- regsubsets.fitted.values(X, reg, k)
  residual_sum_of_squares [k] <- rss(y, yhat)
}
k <- c(1:100)
k_rss <- as.data.frame(cbind(k, residual_sum_of_squares))
# plot RSS as a function of k (number of basis functions)
ggplot(k_rss, aes(x = k, y = residual_sum_of_squares )) + geom_line()
```

## Problem 1 (d)
```{r 1d}
GCV <- rep(0, 100)
# use RSS to get the GCV scores
for (k in 1:100){
  GCV[k] <- (residual_sum_of_squares[k] / (1 - k/100)^2)/100
}
# note: when k = 100, GCV will blow up to infinity
k <- c(1:100)
k_rss <- cbind(k_rss, GCV)
# plot GCV scores as a function of k (number of basis functions)
ggplot(k_rss, aes(x = k, y = GCV)) + geom_line()
```

It is not surpirsing that the GCV score is monotonically decreasing as the number of basis function goes up (except k = 100), because RSS is monotonically decreasing as we use more basis functions to fit the data, and RSS decreases faster the increase of the denominator of the GCV score. However, this curve is less smooth than the previous curve (in 1(c)), because each additional basis function explains different variations relative to the increase of the denominator, which act as the penalty for an additional basis function.

## Problem 1 (e)
```{r 1e}
modified_GCV <- rep(0, 100)
# use RSS to get the modified GCV scores
for (k in 1:100){
  modified_GCV[k] <- (residual_sum_of_squares[k] / (1 - 3*k/100)^2)/100
}
k <- c(1:100)
k_rss <- cbind(k_rss, modified_GCV)
# plot modified GCV scores as a function of k
ggplot(k_rss, aes(x = k, y = modified_GCV)) + geom_line()
```

It is not surprsing that the graph looks very different from the previous graph (in 1(d)), because now we increase the penalty three times for each additional basis function. Thus, if the additional basis function cannot explain much variation, the modified GCV score will increase dramatically. For example, the peak at k = 31 may imply the 31st basis function only explains little variation.

## Problem 1 (f)
```{r 1f}
# set k = 25 as the upper limit for # of basis functions
for_r <- rep(0, 25)
back_r <- rep(0, 25)
for (m in 1:25){
  forward_r <- regsubsets(X, y, method = "forward", nvmax = m, intercept = F)
  backward_r <- regsubsets(X, y, method = "backward", nvmax = m, intercept = F)
  for_yhat <- regsubsets.fitted.values(X, forward_r, m)
  back_yhat <- regsubsets.fitted.values(X, backward_r, m)
  for_r[m] <- rss(y, for_yhat)
  back_r[m] <- rss(y, back_yhat)
}
for_modified_gcv <- rep(0, 25)
back_modified_gcv <- rep(0, 25)
for (m in 1:25){
  for_modified_gcv[m] <- (for_r[m] / (1 - 3*m/100)^2)/100
  back_modified_gcv[m] <- (back_r[m] / (1 - 3*m/100)^2)/100
}
k <- c(1:25)
k_fb <- cbind(k, for_modified_gcv, back_modified_gcv)
k_fb <- as.data.frame(k_fb)
ggplot(k_fb, aes(x = k, y = for_modified_gcv)) + geom_line()
```

\newpage

```{r 1f more}
ggplot(k_fb, aes(x = k, y = back_modified_gcv)) + geom_line()
# smallest modified GCV score for forward and backward models
min(for_modified_gcv) # achieved at k = 5
min(back_modified_gcv) # acieved also at k = 5
```

\newpage

## Problem 2 (b)
```{r 2b}
# Ridge regression
library(glmnet)
lambda_vector <- c(10^6, 10, 1, 0)
ridge <- glmnet(X, y, alpha = 0, lambda = lambda_vector)
coef_matrix <- coef(ridge)[-1,]
prediction <- X %*% coef_matrix
data <- cbind(x, y, prediction)
```

```{r}
# lambda = 10^6 (very close to a horizontional line)
plot(x, y)
lines(x, prediction[,1], lwd=2, col="orange")
```

\newpage

```{r}
# lambda = 10
plot(x, y)
lines(x, prediction[,2], lwd=2, col="orange")
```

\newpage

```{r}
# lambda = 1
plot(x, y)
lines(x, prediction[,3], lwd=2, col="orange")
```

\newpage

```{r}
# lambda = 0
plot(x, y)
lines(x, prediction[,4], lwd=2, col="orange")
```

\newpage


## Problem 2 (c)
```{r 2c}
# cross validation (using 10-fold as default)
# find the optimal lambda between 10^-2 and 10^6
lambdas <- 10^seq(6, -2, by = -.1)
cv.fit <- cv.glmnet(X, y, alpha = 0, lambda = lambdas)
plot(cv.fit)
optimal_lambda <- cv.fit$lambda.min
optimal_lambda
```

\newpage

```{r}
# plot corresponding spline for optimal_lambda
optimal_fit <- glmnet(X, y, alpha=0, lambda = c(optimal_lambda))
optimal_coef_matrix <- coef(optimal_fit)[-1,]
optimal_prediction <- X %*% optimal_coef_matrix
plot(x, y)
lines(x, optimal_prediction, lwd=2, col="orange")
```

